{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wxu/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import sframe\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "import functools\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product = pd.read_csv(\"/Users/wxu/workspace/ML-UW-Coursera/classification/week2/amazon_baby_subset.csv\",header=0)\n",
    "imptWord = json.load(open(\"/Users/wxu/workspace/ML-UW-Coursera/classification/week2/important_words.json\",\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(None,string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product = product.fillna({\"review\":''})\n",
    "product[\"review_clean\"] = product[\"review\"].apply(remove_punctuation)\n",
    "for word in imptWord:\n",
    "    product[word] = product['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(df,features,label):\n",
    "    # df: a data frame that need to be transformed\n",
    "    # features: a list of strings for the features name\n",
    "    # label : single string for the class labels\n",
    "    df[\"constant\"] = 1\n",
    "    features = ['constant'] + features\n",
    "    features_frame = df[features]\n",
    "    features_matrix = features_frame.as_matrix()\n",
    "    label_sarray = df[label]\n",
    "    label_array= label_sarray.as_matrix()\n",
    "    return(features_matrix,label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainIdx = json.load(open(\"/Users/wxu/workspace/ML-UW-Coursera/classification/week7/train.json\",\"r\"))\n",
    "validIdx = json.load(open(\"/Users/wxu/workspace/ML-UW-Coursera/classification/week7/valid.json\",\"r\"))\n",
    "train = product.iloc[trainIdx]\n",
    "valid = product.iloc[validIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wxu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "feature_matrix_train, sentiment_train = get_numpy_data(train, imptWord, 'sentiment')\n",
    "feature_matrix_valid, sentiment_valid = get_numpy_data(valid, imptWord, 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_prob(feature_matrix,coefficient):\n",
    "    dotProduct = np.dot(feature_matrix,coefficient)\n",
    "    prob = 1/(1+np.exp(-dotProduct))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative(feature,errors):\n",
    "    derivative = np.dot(feature,errors)\n",
    "    return(derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_avg_log_likelihood(feature_matrix, sentiment, coefficients):\n",
    "\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    logexp = np.log(1. + np.exp(-scores))\n",
    "    \n",
    "    # Simple check to prevent overflow\n",
    "    mask = np.isinf(logexp)\n",
    "    logexp[mask] = -scores[mask]   \n",
    "    \n",
    "    lp = np.sum((indicator-1)*scores - logexp)/len(feature_matrix)   \n",
    "    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient single data point: 0.0\n",
      "           --> Should print 0.0\n"
     ]
    }
   ],
   "source": [
    "j = 1                        # Feature number\n",
    "i = 10                       # Data point number\n",
    "coefficients = np.zeros(194) # A point w at which we are computing the gradient.\n",
    "\n",
    "predictions = predict_prob(feature_matrix_train[i:i+1,:], coefficients)\n",
    "indicator = (sentiment_train[i:i+1]==+1)\n",
    "\n",
    "errors = indicator - predictions\n",
    "gradient_single_data_point = feature_derivative(errors, feature_matrix_train[i:i+1,j])\n",
    "print \"Gradient single data point: %s\" % gradient_single_data_point\n",
    "print \"           --> Should print 0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient mini-batch data points: 1.0\n",
      "                --> Should print 1.0\n"
     ]
    }
   ],
   "source": [
    "j = 1                        # Feature number\n",
    "i = 10                       # Data point start\n",
    "B = 10                       # Mini-batch size\n",
    "coefficients = np.zeros(194) # A point w at which we are computing the gradient.\n",
    "\n",
    "predictions = predict_prob(feature_matrix_train[i:i+B,:], coefficients)\n",
    "indicator = (sentiment_train[i:i+B]==+1)\n",
    "\n",
    "errors = indicator - predictions\n",
    "gradient_mini_batch = feature_derivative(errors, feature_matrix_train[i:i+B,j])\n",
    "print \"Gradient mini-batch data points: %s\" % gradient_mini_batch\n",
    "print \"                --> Should print 1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_SG(feature_matrix, sentiment, initial_coefficients, step_size, batch_size, max_iter):\n",
    "    log_likelihood_all = []\n",
    "\n",
    "    # make sure it's a numpy array\n",
    "    coefficients = np.array(initial_coefficients)\n",
    "    # set seed=1 to produce consistent results\n",
    "    np.random.seed(seed=1)\n",
    "    # Shuffle the data before starting\n",
    "    permutation = np.random.permutation(len(feature_matrix))\n",
    "    feature_matrix = feature_matrix[permutation,:]\n",
    "    sentiment = sentiment[permutation]\n",
    "\n",
    "    i = 0 # index of current batch\n",
    "    # Do a linear scan over data\n",
    "    for itr in xrange(max_iter):\n",
    "        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n",
    "        # Make sure to slice the i-th row of feature_matrix with [i:i+batch_size,:]\n",
    "        ### YOUR CODE HERE\n",
    "        predictions = predict_prob(feature_matrix_train[i:i+batch_size,:], coefficients)\n",
    "\n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        # Make sure to slice the i-th entry with [i:i+batch_size]\n",
    "        ### YOUR CODE HERE\n",
    "        indicator = (sentiment_train[i:i+batch_size]==+1)\n",
    "\n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        for j in xrange(len(coefficients)): # loop over each coefficient\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j]\n",
    "            # Compute the derivative for coefficients[j] and save it to derivative.\n",
    "            # Make sure to slice the i-th row of feature_matrix with [i:i+batch_size,j]\n",
    "            ### YOUR CODE HERE\n",
    "            derivative = feature_derivative(errors, feature_matrix_train[i:i+batch_size,j])\n",
    "                  # Compute the product of the step size, the derivative, and\n",
    "            # the **normalization constant** (1./batch_size)\n",
    "            ### YOUR CODE HERE\n",
    "            coefficients[j] += coefficients[j]+step_size*derivative\n",
    "\n",
    "        # Checking whether log likelihood is increasing\n",
    "        # Print the log likelihood over the *current batch*\n",
    "        lp = compute_avg_log_likelihood(feature_matrix[i:i+batch_size,:], sentiment[i:i+batch_size],\n",
    "                                        coefficients)\n",
    "        log_likelihood_all.append(lp)\n",
    "        if itr <= 15 or (itr <= 1000 and itr % 100 == 0) or (itr <= 10000 and itr % 1000 == 0) \\\n",
    "         or itr % 10000 == 0 or itr == max_iter-1:\n",
    "            data_size = len(feature_matrix)\n",
    "            print 'Iteration %*d: Average log likelihood (of data points  [%0*d:%0*d]) = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, \\\n",
    "                 int(np.ceil(np.log10(data_size))), i, \\\n",
    "                 int(np.ceil(np.log10(data_size))), i+batch_size, lp)  \n",
    " # if we made a complete pass over data, shuffle and restart\n",
    "        i += batch_size\n",
    "        if i+batch_size > len(feature_matrix):\n",
    "            permutation = np.random.permutation(len(feature_matrix))\n",
    "            feature_matrix = feature_matrix[permutation,:]\n",
    "            sentiment = sentiment[permutation]\n",
    "            i = 0                \n",
    "\n",
    "    # We return the list of log likelihoods for plotting purposes.\n",
    "    return coefficients, log_likelihood_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47780"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Average log likelihood (of data points  [00000:00001]) = -0.82593942\n",
      "Iteration 1: Average log likelihood (of data points  [00001:00002]) = -0.14233761\n",
      "Iteration 2: Average log likelihood (of data points  [00002:00003]) = -0.02752287\n",
      "Iteration 3: Average log likelihood (of data points  [00003:00004]) = -0.00008882\n",
      "Iteration 4: Average log likelihood (of data points  [00004:00005]) = -0.00000001\n",
      "Iteration 5: Average log likelihood (of data points  [00005:00006]) = -0.00000000\n",
      "Iteration 6: Average log likelihood (of data points  [00006:00007]) = 0.00000000\n",
      "Iteration 7: Average log likelihood (of data points  [00007:00008]) = 0.00000000\n",
      "Iteration 8: Average log likelihood (of data points  [00008:00009]) = 0.00000000\n",
      "Iteration 9: Average log likelihood (of data points  [00009:00010]) = -537.72012011\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "iter10 = logistic_regression_SG(feature_matrix_train, sentiment_train, \n",
    "                                initial_coefficients= np.zeros(194), step_size = 5e-1, batch_size=1, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   0: Average log likelihood (of data points  [00000:47780]) = -265.69276346\n",
      "Iteration   1: Average log likelihood (of data points  [00000:47780]) = -4084.36067908\n",
      "Iteration   2: Average log likelihood (of data points  [00000:47780]) = -5599.67602592\n",
      "Iteration   3: Average log likelihood (of data points  [00000:47780]) = -1638.13064401\n",
      "Iteration   4: Average log likelihood (of data points  [00000:47780]) = -3103.35755173\n",
      "Iteration   5: Average log likelihood (of data points  [00000:47780]) = -6069.28643523\n",
      "Iteration   6: Average log likelihood (of data points  [00000:47780]) = -12005.44330972\n",
      "Iteration   7: Average log likelihood (of data points  [00000:47780]) = -23879.62467061\n",
      "Iteration   8: Average log likelihood (of data points  [00000:47780]) = -47628.69694555\n",
      "Iteration   9: Average log likelihood (of data points  [00000:47780]) = -95127.21801709\n",
      "Iteration  10: Average log likelihood (of data points  [00000:47780]) = -190124.45973319\n",
      "Iteration  11: Average log likelihood (of data points  [00000:47780]) = -380119.10496865\n",
      "Iteration  12: Average log likelihood (of data points  [00000:47780]) = -760108.45428439\n",
      "Iteration  13: Average log likelihood (of data points  [00000:47780]) = -1520087.15995011\n",
      "Iteration  14: Average log likelihood (of data points  [00000:47780]) = -3040044.57842878\n",
      "Iteration  15: Average log likelihood (of data points  [00000:47780]) = -6079959.42998439\n",
      "Iteration 100: Average log likelihood (of data points  [00000:47780]) = -235202019422435144199656431419392.00000000\n",
      "Iteration 199: Average log likelihood (of data points  [00000:47780]) = -149076990547870767228726582186001441325068258582146579758579712.00000000\n"
     ]
    }
   ],
   "source": [
    "iter200 = logistic_regression_SG(feature_matrix_train, sentiment_train, \n",
    "                                initial_coefficients= np.zeros(194), step_size = 5e-1, batch_size=feature_matrix_train.shape[0], max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def make_plot(log_likelihood_all, len_data, batch_size, smoothing_window=1, label=''):\n",
    "    plt.rcParams.update({'figure.figsize': (9,5)})\n",
    "    log_likelihood_all_ma = np.convolve(np.array(log_likelihood_all), \\\n",
    "                                        np.ones((smoothing_window,))/smoothing_window, mode='valid')\n",
    "\n",
    "    plt.plot(np.array(range(smoothing_window-1, len(log_likelihood_all)))*float(batch_size)/len_data,\n",
    "             log_likelihood_all_ma, linewidth=4.0, label=label)\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('# of passes over data')\n",
    "    plt.ylabel('Average log likelihood per data point')\n",
    "    plt.legend(loc='lower right', prop={'size':14})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_plot(log_likelihood_all, len_data, batch_size, smoothing_window=1, label='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
